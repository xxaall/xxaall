{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ssss 7.0 - ResNet 50",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JbbIlh7WAE-M",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2UXCV-PSVzOb",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xDLLo7OmVzOe",
        "colab": {}
      },
      "source": [
        "# change the path to read\n",
        "path = '/content/drive/My Drive'\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nY-LhiS60NA9",
        "colab": {}
      },
      "source": [
        "# Image Preparation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k5nVjCpuVzOh",
        "colab": {}
      },
      "source": [
        "# extract features \n",
        "def extract_features(dataset):\n",
        "    # load the model\n",
        "    model = ResNet50() # change to ResNet 50\n",
        "    # re-structure the model \n",
        "    model.layers.pop()  # delete the last/predict layer\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
        "                      # initialize a functional model (get the output of the middle layer)\n",
        "    # summarize (output the parameters of each layer of the model)\n",
        "    print(model.summary()) \n",
        "    # extract features\n",
        "    features = dict() \n",
        "    for name in os.listdir(dataset): # return a list of files under the specified path\n",
        "        # load an image from file\n",
        "        filename = dataset + '/' + name\n",
        "        image = load_img(filename, target_size=(224, 224, 3))\n",
        "        # convert the image pixels to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        # reshape data for the model \n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\n",
        "        # prepare the image for the pre-trained model\n",
        "        image = preprocess_input(image) # normalization (per-example mean subtraction) \n",
        "                                        # to improve the operation of the algorithm\n",
        "\n",
        "        # get features\n",
        "        feature = model.predict(image, verbose=0).flatten()\n",
        "                            # verbose\n",
        "        # get image id\n",
        "        image_id = name.split('.')[0]\n",
        "        # return features\n",
        "        features[image_id] = feature\n",
        "        # print('>%s' % name)\n",
        "    return features\n",
        "\n",
        "# extract features from all images\n",
        "dataset = 'Images' # change \n",
        "features = extract_features(dataset)\n",
        "print('Extracted Features: %d' % len(features)) # output: 8091\n",
        "\n",
        "# save to file\n",
        "from pickle import dump\n",
        "dump(features, open('features2.pkl', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4UwygSpLVzOj",
        "colab": {}
      },
      "source": [
        "# Text Preparation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2psOu9WxVzOm",
        "colab": {}
      },
      "source": [
        "# load text into memory\n",
        "filename = 'Flickr8k_text/Flickr8k.token.txt' \n",
        "# open the file as read only\n",
        "file = open(filename,'r')\n",
        "# read all text\n",
        "text = file.read()\n",
        "# close the file\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEOuqByVzOo",
        "colab": {}
      },
      "source": [
        "## Extract descriptions "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a59ijfuvVzOq",
        "colab": {}
      },
      "source": [
        "datatxt = []\n",
        "for line in text.split('\\n'):  \n",
        "    line0 = line.split('\\t')    \n",
        "    if len(line0) == 1:\n",
        "        continue\n",
        "    w = line0[0].split('#')  \n",
        "    datatxt.append(w + [line0[1].lower()])\n",
        "# change to not capital writing\n",
        "# datatxt contain n list0\n",
        "df_txt = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"desc\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3m169k1SVzOt",
        "colab": {}
      },
      "source": [
        "## Clean descriptions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9iOox_YqVzOv",
        "colab": {}
      },
      "source": [
        "# define the text_clean functions\n",
        "import string\n",
        "def remove_punctuation(text_original):\n",
        "    text_no_punctuation = text_original.translate(str.maketrans('','',string.punctuation))\n",
        "    return(text_no_punctuation)\n",
        "\n",
        "def remove_single_character(text):\n",
        "    text_len_more_than1 = ''\n",
        "    for word in text.split(): # split word by white space/tokenize\n",
        "        if len(word) > 1:\n",
        "            text_len_more_than1 += ' ' + word\n",
        "    return(text_len_more_than1)\n",
        "\n",
        "def remove_numeric(text): \n",
        "    text_no_numeric = ''\n",
        "    for word in text.split(): \n",
        "        isalpha = word.isalpha()\n",
        "        if isalpha is False:\n",
        "            print('{:10}: {:}'.format(word,isalpha)) # e.g. output: 111    : False\n",
        "        if isalpha:  # if isalpha is True\n",
        "            text_no_numeric += ' ' + word\n",
        "    return(text_no_numeric)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2IbC6IeeVzOx",
        "colab": {}
      },
      "source": [
        "# clean descriptions\n",
        "def text_clean(text_original):\n",
        "    text = remove_punctuation(text_original)\n",
        "    text = remove_single_character(text)\n",
        "    text = remove_numeric(text)\n",
        "    return(text)\n",
        "\n",
        "\n",
        "for i, desc in enumerate(df_txt.desc.values): \n",
        "    desc_new = text_clean(desc)\n",
        "    df_txt['desc'].iloc[i] = desc_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n95i9ZUYVzO1",
        "colab": {}
      },
      "source": [
        "## Add start and end sequence tokens (prepare for LSTM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZWNnX_IVzO2",
        "colab": {}
      },
      "source": [
        "from copy import copy\n",
        "def add_start_end_seq_token(text):\n",
        "    desc = []\n",
        "    for txt in text:\n",
        "        txt = 'startseq ' + txt + ' endseq'\n",
        "        desc.append(txt)\n",
        "    return(desc)\n",
        "df_txt0 = copy(df_txt)\n",
        "df_txt0['desc'] = add_start_end_seq_token(df_txt['desc'])\n",
        "# preview\n",
        "df_txt0.head(5)\n",
        "# del df_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57T5Z87g4K0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load CNN weights\n",
        "from pickle import load\n",
        "images = load(open('features2.pkl', 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-xF5L5YIVzO9",
        "colab": {}
      },
      "source": [
        "# link image and text\n",
        "df_txt6 = df_txt0\n",
        "dimages, keepindex = [],[]\n",
        "\n",
        "#print(df_txt6[:5])\n",
        "for i in range(5):\n",
        "    locals()['df_txt'+str(i)] = df_txt6.loc[df_txt6['index'].values == str(i),: ]\n",
        "#locals()['df_txt'+str(0)] = df_txt6.loc[df_txt6['index'].values == '0',: ] \n",
        "\n",
        "# preview\n",
        "#print(df_txt0['index'][:5])                                                                                       \n",
        "#print(dtexts[:5])\n",
        "print(df_txt0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg59BzPXOUZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# link image and all the texts\n",
        "def keep(df_txt1):                                                          \n",
        "    for i, name in enumerate(df_txt1.filename):\n",
        "        name = name.split('.')[0]\n",
        "        if name in images.keys():\n",
        "            dimages.append(images[name])\n",
        "            keepindex.append(i)\n",
        "    \n",
        "    return dimages,keepindex\n",
        "\n",
        "for i in range(5):\n",
        "    locals()['dimages'+str(i)],locals()['keepindex'+str(i)] = keep(locals()['df_txt'+str(i)])\n",
        "    locals()['fnames'+str(i)] = locals()['df_txt'+str(i)]['filename'].iloc[locals()['keepindex'+str(i)]].values\n",
        "    locals()['dtexts'+str(i)] = locals()['df_txt'+str(i)]['desc'].iloc[locals()['keepindex'+str(i)]].values\n",
        "    locals()['dimages'+str(i)] = np.array(locals()['dimages'+str(i)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JDmzgNYEVzPA",
        "colab": {}
      },
      "source": [
        "# tokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def token(dtexts1):\n",
        "  num_words = 8000\n",
        "  tokenizer = Tokenizer(num_words = 8000)\n",
        "  tokenizer.fit_on_texts(dtexts1)  \n",
        "  vocab_size1 = len(tokenizer.word_index) + 1\n",
        "  print('vocabulary size : {}'.format(vocab_size1))\n",
        "  dtexts1 = tokenizer.texts_to_sequences(dtexts1)\n",
        "  print(dtexts1[:5])\n",
        "  return dtexts1, vocab_size1\n",
        "for i in range(5):\n",
        "    locals()['dtexts'+str(i)],locals()['vocab_size'+str(i)] = token(locals()['dtexts'+str(i)])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QV6ID9bXVzPC",
        "colab": {}
      },
      "source": [
        "# data partition\n",
        "ptest, pval = 0.2, 0.2 \n",
        "\n",
        "def split_test_val(dtexts1):# change the function name here\n",
        "  N = len(dtexts1)\n",
        "  Ntest, Nval = int(N*ptest), int(N*pval) \n",
        "  return(dtexts1[:Ntest], \n",
        "          dtexts1[Ntest:Ntest+Nval],  \n",
        "          dtexts1[Ntest+Nval:])\n",
        "\n",
        "for i in range(5):\n",
        "    locals()['dt_test'+str(i)],locals()['dt_val'+str(i)],locals()['dt_train'+str(i)] = split_test_val(locals()['dtexts'+str(i)])\n",
        "    locals()['di_test'+str(i)],locals()['di_val'+str(i)],locals()['di_train'+str(i)] = split_test_val(locals()['dimages'+str(i)])\n",
        "    locals()['fnm_test'+str(i)],locals()['fnm_val'+str(i)],locals()['fnm_train'+str(i)] = split_test_val(locals()['fnames'+str(i)])\n",
        "    locals()['maxlen'+str(i)] = np.max([len(text) for text in locals()['dtexts'+str(i)]])\n",
        "\n",
        "    print('maxlen',i,locals()['maxlen'+str(i)])\n",
        "    print('vocab_size',i,locals()['vocab_size'+str(i)])\n",
        "\n",
        "maxlenchoose = maxlen1 # get the biggest maxlen\n",
        "vocabchoose = vocab_size0 # get the biggest vocab size\n",
        "#maxlen = np.max([len(text) for text in dtexts])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mqsPxh_rVzPD",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "def preprocessing(dtexts1,dimages): # change the function on maxlen, vocab_size\n",
        "    N = len(dtexts1)\n",
        "    print('descriptions = {}'.format(N))\n",
        "    print('images = {}'.format(N))\n",
        "\n",
        "    assert(N==len(dimages))\n",
        "    Xtext, Ximage, ytext = [],[],[]\n",
        "    for text,image in zip(dtexts1,dimages):\n",
        "        for i in range(1,len(text)):\n",
        "            in_text, out_text = text[:i], text[i]\n",
        "            in_text = pad_sequences([in_text],maxlen = maxlenchoose).flatten() # flatten层: 扁平参数，防止过拟合\n",
        "            out_text = to_categorical(out_text,num_classes = vocabchoose)\n",
        "            Xtext.append(in_text)\n",
        "            Ximage.append(image)\n",
        "            ytext.append(out_text)\n",
        "    Xtext  = np.array(Xtext)\n",
        "    Ximage = np.array(Ximage)\n",
        "    ytext  = np.array(ytext)\n",
        "    print('{} {} {}'.format(Xtext.shape,Ximage.shape,ytext.shape))\n",
        "    return(Xtext,Ximage,ytext)\n",
        "\n",
        "# preprocessing is not necessary for testing data\n",
        "\n",
        "# for loop way\n",
        "for i in range(3): # RAM get boom when range to 5.Only 0,1,2 could be used at the same time\n",
        "    print('training')\n",
        "    locals()['Xtext_train'+str(i)],locals()['Ximage_train'+str(i)],locals()['ytext_train'+str(i)] = preprocessing(locals()['dt_train'+str(i)],locals()['di_train'+str(i)])\n",
        "    print('\\ntesting')\n",
        "    locals()['Xtext_val'+str(i)],locals()['Ximage_val'+str(i)],locals()['ytext_val'+str(i)] = preprocessing(locals()['dt_val'+str(i)],locals()['di_val'+str(i)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9gnPMZxOyPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "owAIGwHKsUXM",
        "colab": {}
      },
      "source": [
        "# clean the sessions\n",
        "import keras\n",
        "keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ng4OmUjeVzPM",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "\n",
        "# image feature extractor model\n",
        "input_image = layers.Input(shape=(Ximage_train1.shape[1],)) #4096\n",
        "fimage1 = layers.Dropout(0.5)(input_image)\n",
        "fimage2 = layers.Dense(256,activation='relu',name=\"ImageFeature\")(fimage1)\n",
        "# sequence model\n",
        "input_txt = layers.Input(shape=(maxlenchoose,))\n",
        "# embedding layer must be the first layer of the model\n",
        "stxt1 = layers.Embedding(vocabchoose,256,mask_zero=True)(input_txt)\n",
        "stxt2 = layers.Dropout(0.5)(stxt1)\n",
        "stxt3 = layers.LSTM(256,name=\"TextFeature\")(stxt2)\n",
        "# decoder model\n",
        "decoder1 = layers.add([fimage2,stxt3])\n",
        "decoder2 = layers.Dense(256,activation='relu')(decoder1)\n",
        "decoder3 = layers.Dense(256, activation='relu')(decoder2)\n",
        "outputs = layers.Dense(vocabchoose,activation='softmax')(decoder3)\n",
        "# combine the model \n",
        "model = Model(inputs=[input_image, input_txt],outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam') \n",
        "# summarize\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T4Va4lW9VzPW",
        "colab": {}
      },
      "source": [
        "# Train the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pWIgBoM0VzPa",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "# fit model\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5' # the weight would be save to this file\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "hist = model.fit([Ximage_train2, Xtext_train2], ytext_train2, \n",
        "                  epochs=2, verbose=2, \n",
        "                  batch_size=64, callbacks=[checkpoint],\n",
        "                  validation_data=([Ximage_val2, Xtext_val2], ytext_val2))\n",
        "      \n",
        "end = time.time()\n",
        "print(\"TIME TOOK {:3.2f}MIN\".format((end - start )/60))\n",
        "\n",
        "print(Ximage_train2.shape,Xtext_train2.shape,ytext_train2.shape)\n",
        "# print(hist.history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_oGj5pX_VzPc",
        "colab": {}
      },
      "source": [
        "# visualize the loss\n",
        "for label in [\"loss\",\"val_loss\"]:\n",
        "    plt.plot(hist.history[label],label=label)\n",
        "plt.legend()\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_roCPZK8Xd9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load weights\n",
        "model.load_weights('model-ep002-loss4.075-val_loss3.888.h5') # change to the best weight\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "scores=model.evaluate([Ximage_val2, Xtext_val2],ytext_val2,verbose=0)\n",
        "print(model.metrics_names[1],scores[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iUzvXNq_VzPe",
        "colab": {}
      },
      "source": [
        "# Prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AgY4pvKBVzPg",
        "colab": {}
      },
      "source": [
        "# map an integer to a word\n",
        "index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4tuZDAhzVzPj",
        "colab": {}
      },
      "source": [
        "# alternative\n",
        "def index_word(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Okhblj_2VzPl",
        "colab": {}
      },
      "source": [
        "# generate the descriptions\n",
        "def predict_desc(image):\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(maxlen):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence],maxlen)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image,sequence],verbose=0)\n",
        "        # return the index(integer) of max(yhat)\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = index_word[yhat] # alternative: word = index_word(yhat, tokenizer)\n",
        "        # stop if cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return(in_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gavt8TyMVzPm",
        "colab": {}
      },
      "source": [
        "# demo: show five examples\n",
        "npic = 5\n",
        "npix = 224\n",
        "target_size = (npix,npix,3)\n",
        "\n",
        "count = 1\n",
        "fig = plt.figure(figsize=(10,20))\n",
        "for jpgfnm, image_feature in zip(fnm_test[:npic],di_test[:npic]):\n",
        "    # images \n",
        "    filename = dataset + '/' + jpgfnm\n",
        "    image_load = load_img(filename, target_size=target_size)\n",
        "    ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "    ax.imshow(image_load)\n",
        "    count += 1\n",
        "\n",
        "    # descriptions\n",
        "    desc = predict_desc(image_feature.reshape(1,len(image_feature)))\n",
        "    ax = fig.add_subplot(npic,2,count)\n",
        "    plt.axis('off')\n",
        "    ax.plot()\n",
        "    ax.set_xlim(0,1)\n",
        "    ax.set_ylim(0,1)\n",
        "    ax.text(0,0.5,desc,fontsize=20)\n",
        "    count += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gpgrn2JJVzPq",
        "colab": {}
      },
      "source": [
        "# Evaluate the Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tV7ODpndVzPr",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "# use BLEU for evaluation\n",
        "nkeep = 5\n",
        "pred_good, pred_bad, bleus = [], [], [] \n",
        "count = 0 \n",
        "for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n",
        "    count += 1\n",
        "    if count % 200 == 0:\n",
        "        print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n",
        "    \n",
        "    desc_true = [index_word[i] for i in tokenized_text]     \n",
        "    desc_true = desc_true[1:-1] # remove startseq and endseq\n",
        "  \n",
        "    desc = predict_desc(image_feature.reshape(1,len(image_feature)))\n",
        "    desc = desc.split()\n",
        "    desc = desc[1:-1] # remove startseq and endseq\n",
        "    \n",
        "    bleu = sentence_bleu([desc_true],desc)\n",
        "    bleus.append(bleu)\n",
        "    if bleu > 0.7 and len(pred_good) < nkeep:\n",
        "        pred_good.append((bleu,jpgfnm,desc_true,desc))\n",
        "    elif bleu < 0.3 and len(pred_bad) < nkeep:\n",
        "        pred_bad.append((bleu,jpgfnm,desc_true,desc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iNC1qrqJfnXW",
        "colab": {}
      },
      "source": [
        "print('The average accuracy based on BLEU is:',np.mean(bleus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yd3U0lo9VzPu",
        "colab": {}
      },
      "source": [
        "# demo: show the 'good' and 'bad' results\n",
        "def plot_images(pred):\n",
        "    def create_str(desc_true):\n",
        "        line = \"\"\n",
        "        for s in desc_true:\n",
        "            line += \" \" + s\n",
        "        return(line)\n",
        "    \n",
        "    npix = 224\n",
        "    target_size = (npix,npix,3)    \n",
        "    count = 1\n",
        "    fig = plt.figure(figsize=(10,20))\n",
        "    npic = len(pred)\n",
        "    \n",
        "    for pb in pred:\n",
        "        bleu, jpgfnm, desc_true, desc = pb\n",
        "\n",
        "        filename = dataset + '/' + jpgfnm\n",
        "        image_load = load_img(filename, target_size=target_size) # load_img: 读取指定文件夹下所有images\n",
        "        ax = fig.add_subplot(npic,2,count,xticks=[],yticks=[])\n",
        "        ax.imshow(image_load)\n",
        "        count += 1\n",
        "        \n",
        "        desc_true = create_str(desc_true)\n",
        "        desc = create_str(desc)\n",
        "        \n",
        "        ax = fig.add_subplot(npic,2,count)\n",
        "        plt.axis('off')\n",
        "        ax.plot()\n",
        "        ax.set_xlim(0,1)\n",
        "        ax.set_ylim(0,1)\n",
        "        ax.text(-0.1,0.5,\"Prediction:\" + desc,fontsize=20)\n",
        "        ax.text(-0.1,0.75,\"Original Description:\" + desc_true,fontsize=20)\n",
        "        ax.text(-0.1,0.25,\"Accuray (BLEU): {}\".format(bleu),fontsize=20)\n",
        "        count += 1\n",
        "    plt.show()\n",
        "\n",
        "print(\"Bad Prediction\")\n",
        "plot_images(pred_bad)\n",
        "print(\"Good Prediction\")\n",
        "plot_images(pred_good)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
